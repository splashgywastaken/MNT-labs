{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "498dfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f45aae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество исходных названий: 238708\n"
     ]
    }
   ],
   "source": [
    "# Параметры последовательностей\n",
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "df = pd.read_csv('books.csv', usecols=[\"Book-Title\"], encoding='cp1251')\n",
    "df = df.dropna().drop_duplicates()\n",
    "lines = df['Book-Title'].tolist()\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "print(\"Количество исходных названий:\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d4914c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общий объём символов в тексте: 9052382\n",
      "Число уникальных символов: 140\n"
     ]
    }
   ],
   "source": [
    "# Объединяем все названия в один большой текст\n",
    "text = \"\\n\".join(lines)\n",
    "print(\"Общий объём символов в тексте:\", len(text))\n",
    "\n",
    "# Собираем уникальные символы\n",
    "chars = sorted(list(set(text)))\n",
    "print(\"Число уникальных символов:\", len(chars))\n",
    "\n",
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "596c6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучающих последовательностей: 3017448\n"
     ]
    }
   ],
   "source": [
    "# Формируем обучающие примеры\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print(\"Количество обучающих последовательностей:\", len(sentences))\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8379dbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">137,728</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,060</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m137,728\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)                 │          \u001b[38;5;34m18,060\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,788</span> (608.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m155,788\u001b[0m (608.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">155,788</span> (608.55 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m155,788\u001b[0m (608.55 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Строим модель\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(chars))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(chars), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "# Создадим директорию для сохранения моделей\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "# Коллбеки\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=40, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'models/rano_gen_model.keras',\n",
    "    monitor='loss', \n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df511e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция выборки следующего символа\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds + 1e-10) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02b80c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 20\n",
      "\u001b[1m23574/23574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 34ms/step - loss: 2.0108\n",
      "\n",
      "Генерация текста после эпохи: 1\n",
      "Сид фрагмент: \"Crime Monthly)\n",
      "Criminal Convictions: Err\"\n",
      "\n",
      "Диверсификация: 0.2\n",
      "Сгенерированный текст: or (Silhouette Intimate Moments\n",
      "The Book of Black (Silhouette Intimate Momental Edition (Paperback))\n",
      "The Book of the World (Silhouette Intimate Moments\n",
      "The Sea Story (Harlequin Romance)\n",
      "The Book of the Soul (Harlequin Intrigue\n",
      "The Book of the Companion: The Contemporary Collection (Harlequin Intrigue\n",
      "The Book of the Back (Silhouette Intimate Momental Edition\n",
      "The Sea Short Story (Penguin Series)\n",
      "Th\n",
      "\n",
      "Диверсификация: 0.5\n",
      "Сгенерированный текст: or Book of the Bantasting Good Sychology (Love Special Edition (Paperback))\n",
      "A Novel (Vintage Press Gombar Library)\n",
      "Care and Medical Stories (Baby Sinner Mystery)\n",
      "The Bird of Stail (The Book Book)\n",
      "The Sheit Book of Montand : The Secret Contemporary Works (Paperback))\n",
      "Wild Bears of the Moon (Harlequin Intemperiage)\n",
      "Death of Mart Sea (The Rain\n",
      "Lost Spiritual For Marte (Marines Confupers)\n",
      "The Speaking\n",
      "\n",
      "Диверсификация: 1.0\n",
      "Сгенерированный текст: orking Gandas\n",
      "The Miar Thinfing Club\n",
      "Dream Britesting Fielden: The Ghost Carde for Boad Correlpise\n",
      "The Nunlight Discovery : A Soup-Silhouette Intimate Memelory\n",
      "Is Death Manhasy\n",
      "Concere scensfup in the Bau: Book of Pannates (Penguin Survival (Silhouette In Confuper Romance\n",
      "Teach to Eping the Love (Harlequin Presents Treven)\n",
      "Book of Long Toid\n",
      "Manawick with Help: An Intimate Guide to Storgle\n",
      "Me Love \n",
      "\n",
      "Диверсификация: 1.2\n",
      "Сгенерированный текст: isciradia Y. Vegoped\n",
      "The Glass Work: Shords Cross in Sexualy Telhurer\n",
      "Julontics: A Hanghoatean Woman: You AwaguestaklГсSplaites (La Libotures Sacret Girgс)\n",
      "Pointire: In Eritary of Misim (Mountainakey Traverent Wicae S) (Camfiesautimonpon Ewhutan\n",
      "Gike Pittersel-The Wickwach toly\n",
      "John Mant Blue You VALRAW: SToriey SciencesSate (Drower Alley Ind Blings Norancoa\n",
      "Price Surviva\n",
      "Sweet Jel's Desire: A Ple\n",
      "\n",
      "Epoch: 2 / 20\n",
      "\u001b[1m23574/23574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m828s\u001b[0m 35ms/step - loss: 1.6782\n",
      "\n",
      "Генерация текста после эпохи: 2\n",
      "Сид фрагмент: \"Library)\n",
      "The Cambridge Encyclopedia of t\"\n",
      "\n",
      "Диверсификация: 0.2\n",
      "Сгенерированный текст: he Success (Harlequin Superromance\n",
      "The Complete Poems of the Complete Guide to Story of the Complete Man (Harlequin Presents\n",
      "The Complete Guide to the Complete Guide to the Success of the Contemporary of the World of the Walk of the Complete Guide to the Complete Contemporary Country Companiog\n",
      "The Complete Trip Country of the Contemporary Country Companiog\n",
      "The Complete Guide to the Story of the Wo\n",
      "\n",
      "Диверсификация: 0.5\n",
      "Сгенерированный текст: he Grave (Harlequin Superromances)\n",
      "The Book of Penguin How to Women (And Country Through the Book of Show Mystery (Harlequin Presents\n",
      "The House (Harlequin Presents)\n",
      "Man and Man (Harlequin Temptation\n",
      "The Trade Iver\n",
      "The Decorative Caperaubland: A Pennition of the Midnow (A Don of Century For the Marriage Series\n",
      "The Candid Good Holforme\n",
      "Cassan Masche (Fiction Companyon True Series\n",
      "Raise Aller (Harleq\n",
      "\n",
      "Диверсификация: 1.0\n",
      "Сгенерированный текст: he Chalding Farist Trip \n",
      "Overren America Adson\n",
      "Field Time\n",
      "The Gospelless and uch-Mind Yous Illutimics\n",
      "The Thwow and Revolution: Emerociming Survivalpeing for the World Hilction Whool\n",
      "Compence Way or High Still\n",
      "The Amezit: The Ways the Mart of Women\n",
      "The Christ No\n",
      "Et Gita)\n",
      "De Bhe of the Buffighter\n",
      "Ghost Confection (Dream Love Mysteries\n",
      "Whorasico Up The Sugfion\n",
      "Women's How : A Guide to the Fear Jaton\n",
      "\n",
      "Диверсификация: 1.2\n",
      "Сгенерированный текст: he Favousts\n",
      "Landle - Essenty\n",
      "Whiterary Wrep\n",
      "Tendrams ;)\n",
      "LORN MLTS Ѕchpi: Be&amprenity : A Story of I for MГ¤sure\n",
      "Dase Packair sxphil-Hhill's Prehis (Top Lughtssorie Loftg)}s\n",
      "The M. Issaded (Penguin The 30piel 1\n",
      "T's Deepong\n",
      "Schmoumnd Tale\n",
      "Letter Women\n",
      "The Dublican's Summer\n",
      "Karlops: A Motha's Gso Summer the Pract&rics of the\n",
      "The Talespapene's Verolish's Dundred Ned Writers O'Be Yor Legaz\n",
      "The Wrament\n",
      "\n",
      "Epoch: 3 / 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs)\n\u001b[1;32m----> 7\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      8\u001b[0m         x, y, \n\u001b[0;32m      9\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m     10\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m     11\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, model_checkpoint]\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/rano_gen_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Генерируем текст после эпохи\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:282\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    275\u001b[0m     (\n\u001b[0;32m    276\u001b[0m         val_x,\n\u001b[0;32m    277\u001b[0m         val_y,\n\u001b[0;32m    278\u001b[0m         val_sample_weight,\n\u001b[0;32m    279\u001b[0m     ) \u001b[38;5;241m=\u001b[39m data_adapter_utils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(validation_data)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Create an iterator that yields batches for one epoch.\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    283\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    284\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    285\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    286\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    287\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m    288\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    289\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weight,\n\u001b[0;32m    290\u001b[0m     distribute_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    291\u001b[0m     steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution,\n\u001b[0;32m    292\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_symbolic_build(iterator\u001b[38;5;241m=\u001b[39mepoch_iterator)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:666\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 666\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    668\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    669\u001b[0m         dataset\n\u001b[0;32m    670\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:675\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter\u001b[38;5;241m.\u001b[39mget_tf_dataset()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:236\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    234\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[1;32m--> 236\u001b[0m dataset \u001b[38;5;241m=\u001b[39m slice_inputs(indices_dataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs)\n\u001b[0;32m    238\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n\u001b[0;32m    239\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    240\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[0;32m    241\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:197\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[1;34m(indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    191\u001b[0m inputs \u001b[38;5;241m=\u001b[39m array_slicing\u001b[38;5;241m.\u001b[39mconvert_to_sliceable(\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs, target_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    194\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mlists_to_tuples(inputs)\n\u001b[0;32m    196\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[1;32m--> 197\u001b[0m     (indices_dataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(inputs)\u001b[38;5;241m.\u001b[39mrepeat())\n\u001b[0;32m    198\u001b[0m )\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_one\u001b[39m(x):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:741\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m from_tensors_op\u001b[38;5;241m.\u001b[39m_from_tensors(tensors, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensors_op.py:23\u001b[0m, in \u001b[0;36m_from_tensors\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _TensorDataset(tensors, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensors_op.py:31\u001b[0m, in \u001b[0;36m_TensorDataset.__init__\u001b[1;34m(self, element, name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `tf.data.Dataset.from_tensors` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m   element \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mnormalize_element(element)\n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 134\u001b[0m             ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:732\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    731\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 732\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m    733\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[0;32m    734\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39mconstant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    277\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[0;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Обучение модели и генерация текста после каждой эпохи\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch + 1, \"/\", epochs)\n",
    "    model.fit(\n",
    "        x, y, \n",
    "        batch_size=batch_size, \n",
    "        epochs=1, \n",
    "        callbacks=[early_stopping, model_checkpoint]\n",
    "    )\n",
    "    model.load_weights('models/rano_gen_model.keras')\n",
    "\n",
    "    # Генерируем текст после эпохи\n",
    "    print(\"\\nГенерация текста после эпохи:\", epoch + 1)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    seed = text[start_index : start_index + maxlen]\n",
    "    print('Сид фрагмент: \"' + seed + '\"\\n')\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"Диверсификация:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = seed\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                if char in char_indices:\n",
    "                    x_pred[0, t, char_indices[char]] = 1.0\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "        print(\"Сгенерированный текст:\", generated)\n",
    "        print()\n",
    "\n",
    "# Загрузка лучшей модели после обучения\n",
    "model.load_weights('models/rano_gen_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39793a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
